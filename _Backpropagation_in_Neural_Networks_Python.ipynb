{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation in Neural Networks!"
      ],
      "metadata": {
        "id": "8l-aAtLfTA62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- <small>[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/alokyadavonline/)</small>\n",
        "- <small>[![YouTube](https://img.shields.io/badge/YouTube-Subscribe-red?style=for-the-badge&logo=youtube)](https://youtube.com/@AlokYadavonline?si=rk9LORGLTujPjqOE)</small>\n",
        "\n",
        "<small>Click the \"LinkedIn\" and \"YouTube\" badges above to connect with me on LinkedIn and subscribe to my YouTube channel for the latest updates.</small>"
      ],
      "metadata": {
        "id": "gH2mHsjRS4oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python code demonstrating forward and backward propagation in NumPy\n",
        "\n",
        "\n",
        "**1. Import necessary libraries:**"
      ],
      "metadata": {
        "id": "HuyECKlNFe-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "jH6Zm0-hFe-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Define activation functions and their derivatives:**"
      ],
      "metadata": {
        "id": "XmOSS6vWFe-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return np.where(z > 0, 1, 0)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "id": "e_dvUSkOFe-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Initialize the neural network:**"
      ],
      "metadata": {
        "id": "bHSa_FA8Fe-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the structure of the network\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 3\n",
        "output_layer_size = 1"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "cYHoDyEgFe-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases randomly\n",
        "W1 = np.random.randn(hidden_layer_size, input_layer_size)\n",
        "b1 = np.zeros((hidden_layer_size, 1))\n",
        "W2 = np.random.randn(output_layer_size, hidden_layer_size)\n",
        "b2 = np.zeros((output_layer_size, 1))"
      ],
      "metadata": {
        "id": "mFHGUzXdGcO2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Forward propagation:**"
      ],
      "metadata": {
        "id": "9xLhH2XbFe-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X):\n",
        "    # Calculate the activations for the hidden layer\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)  # Apply ReLU activation\n",
        "\n",
        "    # Calculate the activations for the output layer\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)  # Apply sigmoid activation\n",
        "\n",
        "    return Z1, A1, Z2, A2"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "id": "RmBfS-mRFe-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Backward propagation:**"
      ],
      "metadata": {
        "id": "qLTnXImhFe-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(X, Y, Z1, A1, Z2, A2):\n",
        "    # Calculate the error at the output layer\n",
        "    dZ2 = A2 - Y\n",
        "\n",
        "    # Calculate the error at the hidden layer\n",
        "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)\n",
        "\n",
        "    # Calculate the gradients for the weights and biases\n",
        "    dW2 = np.dot(dZ2, A1.T)\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dW1 = np.dot(dZ1, X.T)\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "id": "gEABo_WwFe-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ODpxt1pHFe-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Example usage:**"
      ],
      "metadata": {
        "id": "7DVA3guaFe-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input and expected output\n",
        "X = np.array([[0.5, 0.1]]).T\n",
        "Y = np.array([[0.7]]).T"
      ],
      "metadata": {
        "id": "FbeQqX8yHBfn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform forward propagation\n",
        "Z1, A1, Z2, A2 = forward_propagation(X)"
      ],
      "metadata": {
        "id": "dpomVIfRHIEs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform backward propagation to calculate the gradients\n",
        "dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, Z2, A2)"
      ],
      "metadata": {
        "id": "9gW0K9BKHJii"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Update weights and biases (using gradient descent):**"
      ],
      "metadata": {
        "id": "bOAWDvOMHqUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update weights and biases\n",
        "learning_rate = 0.1\n",
        "\n",
        "W1 -= learning_rate * dW1\n",
        "b1 -= learning_rate * db1\n",
        "W2 -= learning_rate * dW2\n",
        "b2 -= learning_rate * db2"
      ],
      "metadata": {
        "id": "ZNvBLEpoHPDP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Update weights and biases\n",
        "# ... (as shown above)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "id": "UuB7TRVgFe-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key points:**\n",
        "\n",
        "- **Forward propagation:** Calculates outputs from inputs through the network's layers.\n",
        "- **Backward propagation:** Calculates gradients of the loss function with respect to weights and biases, used for updating them during training.\n",
        "- **NumPy:** Efficiently performs matrix operations essential for neural network computations.\n",
        "- **Activation functions:** Introduce non-linearity, allowing the network to learn complex patterns.\n",
        "- **Gradient descent:** Common optimization algorithm used to update weights and biases in a direction that minimizes the loss function."
      ],
      "metadata": {
        "id": "pw8M67_2Fe-R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJ2fsFZgGC4D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}